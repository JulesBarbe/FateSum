{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import csv\r\n",
    "from datasets import load_dataset\r\n",
    "from textblob import TextBlob\r\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM, BartTokenizer, BartForConditionalGeneration, BartConfig\r\n",
    "from random import sample\r\n",
    "from torch import load\r\n",
    "from time import sleep\r\n",
    "import os\r\n",
    "import keyboard\r\n",
    "import testing"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word lists\n",
    "Custom (and hopefully eventually curated) word lists for finding corpus items that might cause various FATE issues."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# ethnicity related words\r\n",
    "\r\n",
    "# gender and gender identity related words (excluding pronouns)\r\n",
    "gwl = ['woman', 'transexual', 'trans', 'gender', 'transgender', 'asexual', 'non-binary',  'gender-fluid', 'lgbt', 'lgbtq', \"lbtq+\", \"man\", \"feminine\", \"masculine\"] \r\n",
    "\r\n",
    "# plain non-subsective adjectives (Nayak et al., 2014)\r\n",
    "nsawl = ['alleged', 'apparent', 'arguable', 'assumed', 'believed', 'debatable', 'disputed', 'doubtful', 'dubious', 'erroneous', 'expected', 'faulty', 'future', 'historic', 'impossible', 'improbable', 'likely', 'mistaken', 'ostensible', 'plausible', 'possible', 'potential', 'predicted', 'presumed', 'probable', 'putative', 'questionable', 'seeming', 'so-called', 'supposed', 'suspicious', 'theoretical', 'uncertain', 'unlikely', 'unsuccessful']\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Datasets\r\n",
    "Various datasets from different ATS sub-domains. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# CNN/DM -> news\r\n",
    "# Format: split (train,test,validation), features (article, highlights)\r\n",
    "cnn_dm = load_dataset('cnn_dailymail', '3.0.0')\r\n",
    "cnn_dm = cnn_dm['validation']['article'] + cnn_dm['test']['article']\r\n",
    "\r\n",
    "# Reddit TIFU -> blogs\r\n",
    "# Format: split (train), features (ups, num_comments, upvote_ratio, score, documents, tldr)\r\n",
    "tifu = load_dataset('reddit_tifu', 'long')\r\n",
    "\r\n",
    "# SamSum -> dialogue\r\n",
    "# Format: split (train, test, validation), features (id, dialogue, summary)\r\n",
    "samsum = load_dataset('samsum')\r\n",
    "samsum = samsum['validation']['dialogue'] + samsum['test']['dialogue']"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reusing dataset cnn_dailymail (C:\\Users\\ajule\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "Reusing dataset reddit_tifu (C:\\Users\\ajule\\.cache\\huggingface\\datasets\\reddit_tifu\\long\\1.1.0\\bb5bea66e93c55965332f70dc693c38b9e3930a16e9e8a1323ef1a2c8a2fcee6)\n",
      "Reusing dataset samsum (C:\\Users\\ajule\\.cache\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Models"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# BART\r\n",
    "# Trained on large CNN/DM dataset\r\n",
    "BART_CNN_TOKENIZER = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\r\n",
    "BART_CNN_MODEL = TFAutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\r\n",
    "\r\n",
    "# Base BART model finetuned on samsum dataset\r\n",
    "BART_samsum_TOKENIZER = AutoTokenizer.from_pretrained(\"lidiya/bart-base-samsum\")\r\n",
    "BART_samsum_MODEL = AutoModelForSeq2SeqLM.from_pretrained(\"lidiya/bart-base-samsum\")\r\n",
    "\r\n",
    "# PEGASUS\r\n",
    "# Finetuned for TIFU dialogue/blogging dataset\r\n",
    "# PEGASUS_TIFU_TOKENIZER = AutoTokenizer.from_pretrained(\"google/pegasus-reddit_tifu\")\r\n",
    "# PEGASUS_TIFU_MODEL = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-reddit_tifu\")\r\n",
    "\r\n",
    "# MatchSum\r\n",
    "#match_model = load('MatchSum_cnndm_bert.ckpt')\r\n",
    "\r\n",
    "# SMMRY (simple extractive algorithm used by the reddit autotldr bot)\r\n",
    "# smmry_api = 'F780F04404'"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def find_items(corpus, wordlist, length=1000): \r\n",
    "    \"\"\"\r\n",
    "    List of strings, List of strings -> List of tuples (corpus item [string], words present [list of strings], sentences where words present [list of strings])\r\n",
    "    Returns a random sample of specified length with items in the corpus that contain words from the given wordlist.\r\n",
    "    \"\"\"\r\n",
    "    res = []\r\n",
    "    for item in sample(corpus, length):\r\n",
    "        keywords = []\r\n",
    "        key_sentences = []\r\n",
    "        sentences = TextBlob(item).sentences\r\n",
    "\r\n",
    "        for sentence in sentences:\r\n",
    "            set_key = False\r\n",
    "            words = sentence.words.lower()\r\n",
    "\r\n",
    "            for word in words:\r\n",
    "                if word in wordlist:\r\n",
    "                    set_key = True \r\n",
    "                    if word not in keywords:\r\n",
    "                        keywords.append(word)\r\n",
    "\r\n",
    "            if set_key == True:\r\n",
    "                key_sentences.append(sentence)\r\n",
    "\r\n",
    "        if len(keywords) != 0:\r\n",
    "            res.append((item, keywords, key_sentences))\r\n",
    "    \r\n",
    "    return res\r\n",
    "\r\n",
    "def print_item(item): \r\n",
    "    \"\"\"\r\n",
    "    Given 4-tuple with text, keywords, key sentences and summary, print them in a readable manner.\r\n",
    "    \"\"\"\r\n",
    "    print(\"Keywords: \", item[1], \"\\nKey sentences: \", item[2], \"\\nSummary: \", item[3], \"\\nText: \", item[0])\r\n",
    "    \r\n",
    "def show_item(item):\r\n",
    "    \"\"\"\r\n",
    "    Given 4-tuple with text, keywords, key sentences and summary, return concatenated string.\r\n",
    "    \"\"\"\r\n",
    "    return \"Keywords: \", item[1], \"\\nKey sentences: \", item[2], \"\\nSummary: \", item[3], \"\\nText: \", item[0]\r\n",
    "\r\n",
    "def peep_item(item_list):\r\n",
    "    \"\"\"\r\n",
    "    Given list of items described above, print first item in list and pop it. \r\n",
    "    Used for qualitative tests to read summaries one after the other.\r\n",
    "    \"\"\"\r\n",
    "    print(\"Items left: \", len(item_list), \"\\n\")\r\n",
    "    if len(item_list) != 0:\r\n",
    "        print_item(item_list.pop(0))\r\n",
    "\r\n",
    "    \r\n",
    "def summ_dialogue(text):\r\n",
    "    summarizer = pipeline(\"summarization\", model=\"lidiya/bart-base-samsum\")\r\n",
    "    return summarizer(text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# samsum item finding\r\n",
    "corpus = samsum\r\n",
    "eitems = find_items(corpus, ewl, len(samsum))\r\n",
    "gitems = find_items(corpus, gwl, len(samsum))\r\n",
    "aitems = find_items(corpus, nsawl, len(samsum))\r\n",
    "\r\n",
    "print('items done')\r\n",
    "print('e: ', len(eitems), 'g: ', len(gitems), 'a: ', len(aitems))\r\n",
    "dial_e_res = []\r\n",
    "dial_g_res = []\r\n",
    "dial_a_res = []"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "items done\n",
      "e:  54 g:  109 a:  95\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# samsum summarisation of found items\r\n",
    "summarizer = pipeline(\"summarization\", model=\"lidiya/bart-base-samsum\")\r\n",
    "for item in eitems:\r\n",
    "    dial_e_res.append((item[0], item[1], item[2], summarizer(item[0])))\r\n",
    "for item in gitems:\r\n",
    "    dial_g_res.append((item[0], item[1], item[2], summarizer(item[0])))\r\n",
    "for item in aitems:\r\n",
    "    dial_a_res.append((item[0], item[1], item[2], summarizer(item[0])))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Python39\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "Your max_length is set to 128, but you input_length is only 57. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 86. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 110. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 64. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 114. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 113. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 64. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 105. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 123. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 62. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 91. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 96. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 104. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 102. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 67. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 125. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 73. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 108. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 114. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 125. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 68. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 125. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 60. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 115. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 110. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 45. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 67. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 109. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 96. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 77. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 109. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 69. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 77. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 37. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 93. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 41. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 108. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 69. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 63. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 60. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 121. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 101. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 81. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 77. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# cnn_dm item finding\r\n",
    "corpus = cnn_dm\r\n",
    "eitems = find_items(corpus, ewl, 400)\r\n",
    "gitems = find_items(corpus, gwl, 400)\r\n",
    "aitems = find_items(corpus, nsawl, 400)\r\n",
    "\r\n",
    "print('items done')\r\n",
    "print('e: ', len(eitems), 'g: ', len(gitems), 'a: ', len(aitems))\r\n",
    "news_e_res = []\r\n",
    "news_g_res = []\r\n",
    "news_a_res = []"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "items done\n",
      "e:  101 g:  154 a:  208\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# cnn_dm summarisation\r\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn')\r\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn', truncation=True)\r\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)\r\n",
    "\r\n",
    "errors = 0\r\n",
    "\r\n",
    "for item in eitems:\r\n",
    "    # for indexing errors due to sequence length\r\n",
    "    try:\r\n",
    "        s = summarizer(item[0])   \r\n",
    "    except IndexError:\r\n",
    "        errors+=1\r\n",
    "        next\r\n",
    "    else:\r\n",
    "        news_e_res.append((item[0], item[1], item[2], s))\r\n",
    "print(\"e done, errors = \", errors)\r\n",
    "for item in gitems:\r\n",
    "    try:\r\n",
    "        s = summarizer(item[0])   \r\n",
    "    except IndexError:\r\n",
    "        errors+=1\r\n",
    "        next\r\n",
    "    else:\r\n",
    "        news_g_res.append((item[0], item[1], item[2], s))\r\n",
    "print(\"g done, errors = \", errors)\r\n",
    "for item in aitems:\r\n",
    "    try:\r\n",
    "        s = summarizer(item[0])   \r\n",
    "    except IndexError:\r\n",
    "        errors+=1\r\n",
    "        next\r\n",
    "    else:\r\n",
    "        news_a_res.append((item[0], item[1], item[2], s))\r\n",
    "print(\"a done, errors = \", errors)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Python39\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1880 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e done, errors =  42\n",
      "g done, errors =  121\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Your max_length is set to 142, but you input_length is only 118. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "a done, errors =  208\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained('facebook/bart-large-cnn')\r\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-cnn', truncation=True)\r\n",
    "summarizer = pipeline('summarization', model=model, tokenizer=tokenizer)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "peep_item(dial_g_res)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Items left:  88 \n",
      "\n",
      "Keywords:  ['man'] \n",
      "Key sentences:  [Sentence(\"Sebastian: Hey man!\")] \n",
      "Summary:  [{'summary_text': 'Sebastian wishes Casper happy birthday. Casper has to set up a meeting and drink a beer.'}] \n",
      "Text:  Sebastian: Hey man! I want to wish you happy birthday!\n",
      "Sebastian: May this day will bring together your closest ones, so You can feel loved. Let the money never stop flowing into your life, and may Your dreams come true! I wish you all the best! Long lasting health, luck, and above all perseverence in all sections of life. I wish you also strength to make a perfect lemonade out of these lemons thrown by life!\n",
      "Casper: Thank you!\n",
      "Casper: Thank you for remembering about me, for you effort to write these kind words. I appreciate this and I already feel happy that I read this.\n",
      "Casper: We have to set up a meeting and drink a beer or 2.\n",
      "Sebastian: Sure! In touch. Happy Birthday!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "peep_item(dial_a_res)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'dial_a_res' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-6d4d8eef31a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpeep_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdial_a_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dial_a_res' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "peep_item(news_e_res)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'peep_item' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-0b86d770e435>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpeep_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnews_e_res\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'peep_item' is not defined"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}