{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from f_sampling import *\n",
    "import f_drop\n",
    "import csv\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "import importlib\n",
    "import pickle\n",
    "from random import sample\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For reloading specific modules after change (necessary with notbooks when modifying imported modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'f_sampling' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5499b7f7453e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# for reloading specific modules after change\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf_sampling\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'f_sampling' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "importlib.reload(f_sampling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rate test 1: based on word choice\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rate_term(data, use_sample = 0, filter_dict = 0):\n",
    "    \"\"\"\n",
    "    Given list of data items with text and summary fields and an optional sample number,\\n\n",
    "    returns a dictionary of statistics for every word present in the data, filtered for words\\n\n",
    "    that appear [filter] or more times in the given dataset. \n",
    "    \"\"\"\n",
    "\n",
    "    if use_sample:\n",
    "        data = sample(data, use_sample)\n",
    "\n",
    "    num_items = len(data)\n",
    "    text_vocab = defaultdict(int)   # all word occurences in text\n",
    "    summ_vocab = defaultdict(int)   # all word occurences in summaries\n",
    "    data_freq = defaultdict(int)    # number of items containing word (text or summary)\n",
    "    doc_freq = defaultdict(int)     # number of documents containing each word\n",
    "    summ_freq = defaultdict(int)    # number of summaries containing each word\n",
    "    doc_term_freq = defaultdict(int)    # sum of term frequencies for each document\n",
    "    summ_term_freq = defaultdict(int)   # sum of term frequencies for each summary\n",
    "    term_drop = defaultdict(int)    # all occurences of word being dropped in summary\n",
    "    drop_freq = defaultdict(int)    # number of documents where word is dropped\n",
    "    term_add = defaultdict(int)     # all occurences of word being added in summary\n",
    "    add_freq = defaultdict(int)     # number of documents where word is added   \n",
    "\n",
    "    for item in data:\n",
    "        text, summ = nlp(item[\"text\"]), nlp(item[\"summary_text\"])\n",
    "        \n",
    "        word_appears = defaultdict(lambda: False)\n",
    "\n",
    "        # first index for counting occurences, second for knowing if word already seen\n",
    "        curr_summ_vocab = defaultdict(int)\n",
    "        # first index for counting occurences, second for word taken\n",
    "        curr_text_vocab = defaultdict(int)\n",
    "\n",
    "        summ_total_words = 0\n",
    "        text_total_words = 0\n",
    "\n",
    "        # current summary vocab\n",
    "        for word in summ:\n",
    "            if not word.is_punct:\n",
    "                w = word.lemma_\n",
    "                curr_summ_vocab[w] += 1 # increment current summ vocab\n",
    "                summ_vocab[w] += 1  # increment global summ vocab\n",
    "                summ_total_words += 1   \n",
    "        \n",
    "        # current document vocab\n",
    "        for word in text:\n",
    "            if not word.is_punct:\n",
    "                w = word.lemma_\n",
    "                curr_text_vocab[w] += 1 # increment current text vocab\n",
    "                text_vocab[w] += 1  # increment global text vocab\n",
    "                text_total_words += 1\n",
    "\n",
    "        # current document term frequencies\n",
    "        for key in curr_text_vocab.keys():\n",
    "            curr_key_freq = curr_text_vocab[key]/text_total_words   # calculate term frequency in text\n",
    "            doc_term_freq[key] += curr_key_freq # increment global term frequency\n",
    "            doc_freq[key] += 1 # increment number of documents containing word\n",
    "            # word was dropped in summary\n",
    "            if key not in curr_summ_vocab:\n",
    "                term_drop[key] += curr_text_vocab[key]  # increment drop occurences for term with total occurences in text\n",
    "                drop_freq[key] += 1 # increment number of documents where word dropped\n",
    "            if not word_appears[key]:\n",
    "                data_freq[key] += 1\n",
    "                word_appears[key] = True\n",
    "\n",
    "        # current summary term frequencies\n",
    "        for key in curr_summ_vocab.keys():\n",
    "            curr_key_freq = curr_summ_vocab[key]/summ_total_words   # calculate term frequency in summary\n",
    "            summ_term_freq[key] += curr_key_freq    # increment global term frequency\n",
    "            summ_freq[key] += 1 # increment number of summaries containing word\n",
    "            # word was added in summary\n",
    "            if key not in curr_text_vocab:\n",
    "                term_add[key] += curr_summ_vocab[key]  # increment add occurences for this term with total summary occurences\n",
    "                add_freq[key] += 1 # increment number of documents where word added\n",
    "            if not word_appears[key]:\n",
    "                data_freq[key] += 1\n",
    "                word_appears[key] = True\n",
    "\n",
    " \n",
    "    res = {}\n",
    "    final_vocab = text_vocab | summ_vocab\n",
    "    for word in final_vocab.keys():\n",
    "        res[word] = {}\n",
    "        res[word][\"corpus frequency\"] = data_freq[word]\n",
    "        res[word][\"document frequency\"] = doc_freq[word]\n",
    "        res[word][\"relative document frequency\"] = doc_freq[word]/num_items\n",
    "        res[word][\"summary frequency\"] = summ_freq[word]\n",
    "        res[word][\"relative summary frequency\"] = summ_freq[word]/num_items\n",
    "        res[word][\"document term frequency\"] = doc_term_freq[word]/num_items\n",
    "        res[word][\"summary term frequency\"] = summ_term_freq[word]/num_items\n",
    "        if text_vocab[word]: \n",
    "            res[word][\"term drop rate\"] = term_drop[word]/text_vocab[word]\n",
    "        else:\n",
    "            res[word][\"term drop rate\"] = \"n/a\"\n",
    "        if doc_freq[word]:\n",
    "            res[word][\"drop rate frequency\"] = drop_freq[word]/doc_freq[word]\n",
    "        else:\n",
    "            res[word][\"drop rate frequency\"] = \"n/a\"\n",
    "        if summ_vocab[word]:\n",
    "            res[word][\"term add rate\"] = term_add[word]/summ_vocab[word]\n",
    "        else:\n",
    "            res[word][\"term add rate\"] = \"n/a\"\n",
    "        if num_items-doc_freq[word]:\n",
    "            res[word][\"add rate frequency\"] = add_freq[word]/(num_items-doc_freq[word])\n",
    "        else:\n",
    "            res[word][\"add rate frequency\"] = \"n/a\"\n",
    "\n",
    "    if filter_dict:\n",
    "        res = dict(filter(lambda elem: elem[1][\"corpus frequency\"] >= filter_dict, res.items()))\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_print_helper(x, orderby):\n",
    "    y = x[1][orderby]\n",
    "    if y != \"n/a\":\n",
    "        return y\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def drop_print(drop_dict, name, method=\"pickle\", orderby=\"term drop rate\", rev=True, filterd=0):\n",
    "\n",
    "    if filterd:\n",
    "        drop_dict = dict(filter(lambda elem: elem[1][\"corpus frequency\"] >= filterd, drop_dict.items()))\n",
    "\n",
    "    if method == \"pickle\":\n",
    "        with open(f\"samples/{name}-f{filterd}\", \"w\") as f:\n",
    "            pickle.dump(drop_dict, f)\n",
    "\n",
    "    if method == \"csv\":\n",
    "        with open(f\"samples/{name}-f{filterd}.csv\", \"w\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            ordered_kv = sorted(drop_dict.items(), key=lambda x: drop_print_helper(x, orderby), reverse=rev)\n",
    "            writer.writerow([\"term\"] + list(ordered_kv[0][1]))\n",
    "            for key, value in ordered_kv:\n",
    "                writer.writerow([key] + list(value.values()))\n",
    "                \n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "xsumpresummext = fopen(\"xsum-presummext-all\")\n",
    "xsumpresummext_res = drop_rate_term(xsumpresummext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_print(xsumpresummext_res, f\"drop-xsum-presummext-{len(xsumpresummext)}\", method=\"csv\", filterd=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop rate test 2: based on sentence position (unfinished, first try did not work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rate_position(data, num_sentences = 30):\n",
    "    sentence_pos = defaultdict(lambda: [0 for _ in range(num_sentences)])\n",
    "    \n",
    "    for item in data:\n",
    "        text, summ = nlp(item[\"text\"]), nlp(item[\"summary_text\"])\n",
    "\n",
    "        for i,sentence in enumerate(text.sents):\n",
    "            \n",
    "            for word in sentence:\n",
    "                if not word.ispunct:\n",
    "                    w = word.lemma_\n",
    "    pass\n",
    "\n",
    "    return sentence_pos\n",
    "          \n",
    "k = drop_rate_position(xsumpeg[:5], num_sentences=10)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
