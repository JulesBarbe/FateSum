{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ajule\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchaudio\\backend\\utils.py:67: UserWarning: No audio backend is available.\n",
      "  warnings.warn('No audio backend is available.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv\n",
    "from datasets import load_dataset\n",
    "from textblob import TextBlob\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSeq2SeqLM, AutoModelForSeq2SeqLM\n",
    "from random import sample\n",
    "from torch import load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word lists\n",
    "Custom (and hopefully eventually curated) word lists for finding corpus items that might cause various FATE issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ethnicity related words\n",
    "ewl = ['black', 'african-american', 'arab', 'mexican', 'asian', 'african', 'korean', 'japanese', 'chinese', 'china', 'korea', 'africa', 'japan', 'mexico', 'nigger', 'nigga', 'terrorist', 'islam', 'hindu', 'india'] \n",
    "\n",
    "# gender and gender identity related words\n",
    "gwl = ['she', 'woman', 'transexual', 'trans', 'gender', 'transgender', 'asexual', 'non-binary',  'gender-fluid', 'lgbt', 'lgbtq', \"lbtq+\" ] \n",
    "\n",
    "# plain non-subsective adjectives (Nayak et al., 2014)\n",
    "nsawl = ['alleged', 'apparent', 'arguable', 'assumed', 'believed', 'debatable', 'disputed', 'doubtful', 'dubious', 'erroneous', 'expected', 'faulty', 'future', 'historic', 'impossible', 'improbable', 'likely', 'mistaken', 'ostensible', 'plausible', 'possible', 'potential', 'predicted', 'presumed', 'probable', 'putative', 'questionable', 'seeming', 'so-called', 'supposed', 'suspicious', 'theoretical', 'uncertain', 'unlikely', 'unsuccessful']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets\n",
    "Various datasets from different ATS sub-domains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (C:\\Users\\ajule\\.cache\\huggingface\\datasets\\cnn_dailymail\\3.0.0\\3.0.0\\3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "Reusing dataset reddit_tifu (C:\\Users\\ajule\\.cache\\huggingface\\datasets\\reddit_tifu\\long\\1.1.0\\bb5bea66e93c55965332f70dc693c38b9e3930a16e9e8a1323ef1a2c8a2fcee6)\n",
      "Reusing dataset samsum (C:\\Users\\ajule\\.cache\\huggingface\\datasets\\samsum\\samsum\\0.0.0\\3f7dba43be72ab10ca66a2e0f8547b3590e96c2bd9f2cbb1f6bb1ec1f1488ba6)\n"
     ]
    }
   ],
   "source": [
    "# CNN/DM -> news\n",
    "# Format: split (train,test,validation), features (article, highlights)\n",
    "cnn_dm = load_dataset('cnn_dailymail', '3.0.0')\n",
    "cnn_dm = cnn_dm['validation']['article'] + cnn_dm['test']['article']\n",
    "\n",
    "# Reddit TIFU -> blogs\n",
    "# Format: split (train), features (ups, num_comments, upvote_ratio, score, documents, tldr)\n",
    "tifu = load_dataset('reddit_tifu', 'long')\n",
    "\n",
    "# SamSum -> dialogue\n",
    "# Format: split (train, test, validation), features (id, dialogue, summary)\n",
    "samsum = load_dataset('samsum')\n",
    "samsum = samsum['validation']['dialogue'] + samsum['test']['dialogue']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBartForConditionalGeneration.\n",
      "\n",
      "All the layers of TFBartForConditionalGeneration were initialized from the model checkpoint at facebook/bart-large-cnn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBartForConditionalGeneration for predictions without further training.\n",
      "Downloading: 100%|██████████| 295/295 [00:00<00:00, 308kB/s]\n",
      "Downloading: 100%|██████████| 1.68k/1.68k [00:00<00:00, 1.68MB/s]\n",
      "Downloading: 100%|██████████| 798k/798k [00:00<00:00, 5.30MB/s]\n",
      "Downloading: 100%|██████████| 456k/456k [00:00<00:00, 4.43MB/s]\n",
      "Downloading: 100%|██████████| 1.36M/1.36M [00:00<00:00, 7.72MB/s]\n",
      "Downloading: 100%|██████████| 239/239 [00:00<00:00, 119kB/s]\n",
      "Downloading: 100%|██████████| 558M/558M [00:42<00:00, 13.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "# BART\n",
    "# Trained on large CNN/DM dataset\n",
    "BART_CNN_TOKENIZER = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "BART_CNN_MODEL = TFAutoModelForSeq2SeqLM.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "# Base BART model finetuned on samsum dataset\n",
    "BART_samsum_TOKENIZER = AutoTokenizer.from_pretrained(\"lidiya/bart-base-samsum\")\n",
    "BART_samsum_MODEL = AutoModelForSeq2SeqLM.from_pretrained(\"lidiya/bart-base-samsum\")\n",
    "\n",
    "# PEGASUS\n",
    "# Finetuned for TIFU dialogue/blogging dataset\n",
    "# PEGASUS_TIFU_TOKENIZER = AutoTokenizer.from_pretrained(\"google/pegasus-reddit_tifu\")\n",
    "# PEGASUS_TIFU_MODEL = AutoModelForSeq2SeqLM.from_pretrained(\"google/pegasus-reddit_tifu\")\n",
    "\n",
    "# MatchSum\n",
    "#match_model = load('MatchSum_cnndm_bert.ckpt')\n",
    "\n",
    "# SMMRY (simple extractive algorithm used by the reddit autotldr bot)\n",
    "# smmry_api = 'F780F04404'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_items(corpus, wordlist, length=1000): \n",
    "    \"\"\"\n",
    "    List of strings, List of strings -> List of tuples (corpus item [string], words present [list of strings], sentences where words present [list of strings])\n",
    "    Returns a random sample of specified length with items in the corpus that contain words from the given wordlist.\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for item in sample(corpus, length):\n",
    "        keywords = []\n",
    "        key_sentences = []\n",
    "        sentences = TextBlob(item).sentences\n",
    "\n",
    "        for sentence in sentences:\n",
    "            set_key = False\n",
    "            words = sentence.words.lower()\n",
    "\n",
    "            for word in words:\n",
    "                if word in wordlist:\n",
    "                    set_key = True \n",
    "                    if word not in keywords:\n",
    "                        keywords.append(word)\n",
    "\n",
    "            if set_key == True:\n",
    "                key_sentences.append(sentence)\n",
    "\n",
    "        if len(keywords) != 0:\n",
    "            res.append((item, keywords, key_sentences))\n",
    "    \n",
    "    return res\n",
    "\n",
    "def print_item(item): \n",
    "    \"\"\"\n",
    "    Given 4-tuple with text, keywords, key sentences and summary, print them in a readable manner.\n",
    "    \"\"\"\n",
    "    print(\"Text: \", item[0], \"\\nKeywords: \", item[1], \"\\nKey sentences: \", item[2], \"\\nSummary: \", item[3])\n",
    "    \n",
    "def show_item(item):\n",
    "    \"\"\"\n",
    "    Given 4-tuple with text, keywords, key sentences and summary, return concatenated string.\n",
    "    \"\"\"\n",
    "    return \"Text: \", item[0], \"\\nKeywords: \", item[1], \"\\nKey sentences: \", item[2], \"\\nSummary: \", item[3]\n",
    "    \n",
    "def summ_dialogue(text):\n",
    "    summarizer = pipeline(\"summarization\", model=\"lidiya/bart-base-samsum\")\n",
    "    return summarizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "items done\n",
      "e:  12 g:  47 a:  29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "corpus = samsum\n",
    "eitems = find_items(corpus, ewl, 400)\n",
    "gitems = find_items(corpus, gwl, 400)\n",
    "aitems = find_items(corpus, nsawl, 400)\n",
    "\n",
    "print('items done')\n",
    "print('e: ', len(eitems), 'g: ', len(gitems), 'a: ', len(aitems))\n",
    "e_res = []\n",
    "g_res = []\n",
    "a_res = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but you input_length is only 65. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'summary_text': 'Lydia is an alleged convict. John and Jack are not sure yet.'}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ex = \"Jack: It ain't right man. She  \\r\\nJohn: She's an alleged convict?! That is crazy but we have to remember it is not sure yet. \\r\\nJack: That is true, let's not jump to conclusions yet.\"\n",
    "\n",
    "print(summ_dialogue(ex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 128, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "C:\\Python39\\lib\\site-packages\\torch\\_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ..\\aten\\src\\ATen\\native\\BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n",
      "Your max_length is set to 128, but you input_length is only 86. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 96. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 111. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 106. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 43. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 56. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 69. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 81. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 27. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 105. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 121. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 69. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 84. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 63. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 81. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n",
      "Your max_length is set to 128, but you input_length is only 90. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=50)\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline(\"summarization\", model=\"lidiya/bart-base-samsum\")\n",
    "for item in eitems:\n",
    "    e_res.append((item[0], item[1], item[2], summarizer(item[0])))\n",
    "for item in gitems:\n",
    "    g_res.append((item[0], item[1], item[2], summarizer(item[0])))\n",
    "for item in aitems:\n",
    "    a_res.append((item[0], item[1], item[2], summarizer(item[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n",
      "Text:  Jeff: Have you heard about this Christmas at Wall Street?\n",
      "Peter: Yup, was apparently the worst since the end of the 19th century\n",
      "Miranda: but it's bounced back since, no panic\n",
      "Jeff: I've listened to some radio podcasts about the American economy\n",
      "Jeff: and it's pretty scary, I mean the economic war on China that Trump is waging and all his unpredictability\n",
      "Jeff: but I'm not an economist\n",
      "Peter: I know, it's hard to say what may happen because Trump is rather unpredictable\n",
      "Peter: But people in my bank seem calm, at least 2019 should be calm\n",
      "Miranda: yup, calm, but the global economy will get weaker\n",
      "Miranda: and the growth will be weaker but not tragic, I suppose\n",
      "Jeff: and the EU? How do you think?\n",
      "Miranda: We just don't know what will happen to the UK and with Brexit \n",
      "Miranda: There are many different scenarios possible\n",
      "Miranda: but it seems that 2020 may be much worse\n",
      "Peter: I agree, I'd expect a global economic meltdown in the early 2020s \n",
      "Keywords:  ['possible'] \n",
      "Key sentences:  [Sentence(\"Miranda: We just don't know what will happen to the UK and with Brexit \n",
      "Miranda: There are many different scenarios possible\n",
      "Miranda: but it seems that 2020 may be much worse\n",
      "Peter: I agree, I'd expect a global economic meltdown in the early 2020s\")] \n",
      "Summary:  [{'summary_text': 'The Christmas at Wall Street was the worst since the end of the 19th century. The economy has bounced back since, but the global economy will get weaker in the early 2020s.'}]\n"
     ]
    }
   ],
   "source": [
    "print(len(a_res))\n",
    "item = a_res[13]\n",
    "print_item(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeEncodeError",
     "evalue": "'charmap' codec can't encode character '\\U0001f601' in position 280: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9924ee8c2c3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dialogue_nsa.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ma_res\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshow_item\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'\\n'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python39\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mencode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalEncoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'charmap' codec can't encode character '\\U0001f601' in position 280: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "with open(\"dialogue_nsa.txt\", 'w') as file:\n",
    "    for item in a_res:\n",
    "        print(show_item(item), '\\n', file=file)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
